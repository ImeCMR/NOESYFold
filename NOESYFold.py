# -*- coding: utf-8 -*-
"""sidechainnet_walkthrough_v3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/178vGN5aMD_gmS0Z4XbFWMbUZu3xHAWmD

This notebook will guide you through several of the functionalities included in SidechainNet including:
1. Loading SidechainNet data as a Python Dictionary,
2. yielding batches for model training with PyTorch DataLoaders,
3. generating structure files and visualizations, and
4. complete model training examples.

# Installation
"""

!pip install sidechainnet

"""# 1. Downloading data with scn.load

The cell below downloads an example version SidechainNet (called `'debug'` because it is smaller in size for demonstration purposes) as a Python dictionary. The structure of the dictionary is shown below. By default, the file is saved in the current directory under `./sidechainnet_data`. The `settings` key shows some metadata.

If you'd like to download a full version of the SidechainNet dataset, replace the `'debug'` argument with the arguments `casp_version=12, thinning=30`.

```python
d  =   {"train": {"seq": [seq1, seq2, ...],  # Sequences
                  "ang": [ang1, ang2, ...],  # Angles
                  "crd": [crd1, crd2, ...],  # Coordinates
                  "evo": [evo1, evo2, ...],  # PSSMs and Information Content
                  "sec": [sec1, sec2, ...],  # Secondary structure (DSSP)
                  "res": [res1, res2, ...],  # X-ray crystallographic resolution
                  "ids": [id1, id2,   ...],  # Corresponding ProteinNet IDs
                  },
        "valid-10": {...},
            ...
        "valid-90": {...},
        "test":     {...},
        "settings": {...},
        "description" : "SidechainNet for CASP 12."
        "date": "September 20, 2020"
        }
```
"""

import sidechainnet as scn
import numpy as np

np.random.seed(0)
d = scn.load("debug")
# d = scn.load(casp_version=12, thinning=30)

"""There are many other options available when loading data. For example, you can specify which CASP competition dataset to load (CASP 7 - 12), as well as the dataset "thinning", which refers to downsampled versions of the data based on sequence similarity (`[30, 50, 70, 90, 95, 100]`, with `100` corresponding to a 100%, complete dataset)."""

# View the documenation for scn.load here
scn.load?

"""# 2. Loading data with DataLoaders for model training

## 2.1 Organization

Requesting DataLoaders returns a dictionary mapping split names to DataLoaders.
"""

d = scn.load("debug",
             with_pytorch='dataloaders',
             dynamic_batching=False,
             batch_size=4)
print("Available Dataloaders =", list(d.keys()))

d['train'].dataset

d['valid-10'].dataset

"""When yielding batches, each dataloader returns a Batch `namedtuple` object, which has the following attributes:


*   `pids` (tuple of ProteinNet/SidechainNet ids for proteins in this batch)
*   `seqs` (tensor of sequences, either as integers or as one-hot vectors depending on value of `scn.load(...seq_as_onehot)`
*  `msks` (tensor of missing residue masks, somewhat redundant with padding in data)
*  `evos` (tensor of PSSM + information content)
*  `secs` (tensor of secondary structure, either as integers or one-hot vectors depending on value of `scn.load(...seq_as_onehot)`
*  `angs` (tensor of angles)
*  `crds` (tensor of coordinates)
* `resolutions` (tuple of X-ray crystallographic resolutions)


"""

for batch in d['train']:
    break

print("Protein IDs\n   ", batch.pids)
print("Sequences\n   ", batch.seqs.shape)
print("Evolutionary Data\n   ", batch.evos.shape)
print("Secondary Structure\n   ", batch.secs.shape)
print("Angle Data\n   ", batch.angs.shape)
print("Coordinate Data\n   ", batch.crds.shape)
print("X-ray Resolution\n   ", batch.resolutions)
print("Concatenated Data (seq/evo/2ndary)\n   ", batch.seq_evo_sec.shape)

"""## 2.2 Integer and 1-hot Sequence Representations

Because some PyTorch functionality is easier to access if you have integer-encoded sequences, DataLoaders yield both `int_seqs` and 1-hot formatted sequences via `seqs`.
"""

for batch in d['train']:
    break
print("Integer sequence")
print("\tShape:", batch.int_seqs.shape)
print("\tEx:", batch.int_seqs[0,:3])

print("1-hot sequence")
print("\tShape:", batch.seqs.shape)
print("\tEx:\n", batch.seqs[0,:3])

"""### A note on sequence padding
Sequences, when expressed as integer sequences, include a specific character (`20`) to represent batch-level padding. This isn't necessary for one-hot vectors, an all-zero vector with the same dimension can represent the same information. This is handled in SidechainNet.
"""

d = scn.load("debug", with_pytorch='dataloaders', dynamic_batching=False,
             batch_size=4)
for batch in d['train']:
    break

# Default integer sequence representation uses the int 20 for padding
# Example: observe the last 15 amino-acid 'characters' of sequence #1
# which has been padded with 20s to match the batch size
batch.int_seqs[1,-15:]

"""We can see that, when batch-level padding is included in a one-hot-encoded sequence, it ends in several zero-vectors."""

d = scn.load("debug", with_pytorch='dataloaders', dynamic_batching=False,
             batch_size=12)
for batch in d['train']:
    break

# By summing over the last dimension of the sequence, we can see
# the last several one-hot vectors contain all zeros for padding
batch.seqs[1, -25:].sum(axis=1)

"""## 2.3 Dynamic Batch Sizes with DataLoaders

With `dynamic_batching=True` (`True` by default), the DataLoader yields batches that attempt to have `batch_size * average_sequence_length` residues in it. Using this soft constraint, the DataLoader yields batches with more proteins when their lengths are small, and batches with fewer proteins when their lengths are large. Yielding proteins of similar length may speed up computation.
"""

d = scn.load("debug", with_pytorch='dataloaders', dynamic_batching=True,
             batch_size=4)
i = 5
for batch in d['train']:
    i -= 1
    if i <= 0:
        break
    print(f"Model Input = {tuple(batch.seq_evo_sec.shape)}; Total residues = "
          f"{batch.seq_evo_sec.shape[0] * batch.seq_evo_sec.shape[1]}.")

"""# 3. Visualizing Structures

## 3.1 Interactive Visualizations with py3Dmol
"""

d = scn.load("debug")

example = 20 # 308, note many indices point to structures that have gaps, and thus cannot be visualzed/constructed from angles
seq, ang, crd, mask, sec = d['train']['seq'][example], d['train']['ang'][example], d['train']['crd'][example], d['train']['msk'][example], d['train']['sec'][example]
res = d['train']['res'][example]
name = d['train']['ids'][example]

print(f"\nExample using {name}.\n")
print(f"Sequence, Mask, and Secondary Structure:\n{seq}\n{mask}\n{sec}\n")
print(f"Angles:\n{ang[:3]} ...\n")
print(f"Coordinates:\n{crd[:3]} ...\n")
print(f"Resolution:\n{res} A")

"""Structures can be built and visualized using *(sequence, angle)* or *(sequence, coordinate)* pairs. **Visualizations are interactive and can be rotated and zoomed with the mouse.**

Due to several assumptions about bond lengths and angles, generating a known structure from its coordinates is always more accurate than generating it from its angles.


"""

sb1 = scn.StructureBuilder(seq, ang)
sb1.to_3Dmol()

sb2 = scn.StructureBuilder(seq, crd)
sb2.to_3Dmol()

"""We can see that after alignment, there is a small error between the angles->coordinate structure. No RMSD error is present for the coordinate-generated structure, since the coordinate values are identical."""

import prody as pr

# Exclude missing residues
mask = sb1.coords.sum(axis=1)!=0
# Align generated coordinates to true coordinates
sb1.coords[mask] = pr.calcTransformation(sb1.coords[mask], crd[mask]).apply(sb1.coords[mask])
# Compute RMSD (reconstruction error)
print(f"RMSD when generated from angles      = {pr.calcRMSD(sb1.coords[mask], crd[mask]):.4f}")
print(f"RMSD when generated from coordinates = {pr.calcRMSD(sb2.coords, crd):.4f}")

"""## 3.2 Generating PDB Files"""

sb1.to_pdb(f"{name}.pdb")

# To demonstrate the underlying PDB file representation
print("\n".join(sb1.pdb_creator._pdb_lines[:6]))

"""## 3.3 Exceptions for Missing Residues

Because coordinates must be generated from a contiguous sequence of angles, if angles are missing for any residues, SidechainNet will raise an error and stop structure generation (see example below).

However, structures with gaps can still be built from coordinates.

Note that this complexity only applies to structure generation when angle data is incomplete. When predicting angle data for structures with gaps, structure generation may still work as the gaps may be filled in by predicted angles.
"""

example = 130
seq, ang, crd = d['train']['seq'][example], d['train']['ang'][example], \
                d['train']['crd'][example]
name = d['train']['ids'][example]
print(f"Example using {name}.")
sb = scn.StructureBuilder(seq, ang)
sb.to_3Dmol()

sb = scn.StructureBuilder(seq, crd)
sb.to_3Dmol()

"""# 4. Model Training Examples

Below you will find some examples of using `sidechainnet` to train machine learning models to predict protein structure (angles or coordinates) from amino acid sequences. These examples are close to what we would consider the minimum required for complete model training.

The cells below train on the `debug` dataset by default. Feel free to modify the call to `scn.load` to select another SidechainNet dataset like CASP12.

For the purposes of this notebook, we have implemented two extremely basic recurrent neural networks to predict angle representations of proteins from their amino acid sequences:


1.   `sidechainnet.examples.models.`**`IntegerSequenceProteinRNN`**
  - takes takes as input amino acid sequences rerpesented as integer tensors ($ℝ^{1}$).
2.   `sidechainnet.examples.models.`**`PSSMProteinRNN`**
  - takes as input a concatenation of the amino acid sequence (one-hot vectors, $ℝ^{20}$), the Position Specific Scoring Matrix ($ℝ^{20}$), and the information content ($ℝ^{1}$).

The output of each neural network is a tensor ($B \times L \times ℝ^{24}$), where $B$ is the batch size, $L$ is the maximum sequence length of proteins within the batch, and $ℝ^{24}$ represents the fact that there are up to 12 angles predicted for each amino acid residue, and each of these 12 angles is decomposed into its `sin` and `cos` components, $(s, c)$. After prediction, the true angle can be recovered with the formula $\chi = \text{atan2}(\sin(s), \cos(c))$.

It's possible to predict the angles directly (set `sincos_output=False`), but that means MSE cannot be used as a loss function, since $-\pi = \pi$. Thus, we demonstrate models that use the $\sin/\cos$ trick for predicting angles for Sections 4.1 and 4.2, whereas 4.3 attemps to predict coordinates/angles directly.

*For finer details about how to load data in different formats, see the above section titled **"Loading data with DataLoaders for model training"**.*


"""

import matplotlib.pyplot as plt
import numpy as np
import torch
from tqdm.notebook import tqdm

from sidechainnet.examples import losses, models
from sidechainnet.structure.structure import inverse_trig_transform

# To train with a GPU, go to Runtime > Change runtime type
if torch.cuda.is_available():
  device = torch.device("cuda")
else:
  device = torch.device("cpu")
print(f"Using {device} for training.")

"""## 4.1 Training with Sequences Alone"""

# Load the data in the appropriate format.
d = scn.load("debug",
            #  casp_version=12,
            #  thinning=30,
             with_pytorch="dataloaders",
             aggregate_model_input=False,
             seq_as_onehot=False,
             batch_size=8,
             dynamic_batching=True)

seqonly_model = models.IntegerSequenceProteinRNN(size=512,
                                                 n_layers=4,
                                                 bidirectional=True,
                                                 device=device)
seqonly_model.to(device)

optimizer = torch.optim.Adam(seqonly_model.parameters())
batch_losses = []
epoch_training_losses = []
epoch_validation10_losses = []
epoch_validation90_losses = []
mse_loss = torch.nn.MSELoss()

def evaluate(model, datasplit):
  """Evaluate a model with (input:integer sequence, output:sin/cos represented
     angles [-1,1]) on LDRMSD. """
  total = 0.0
  n = 0
  with torch.no_grad():
    for batch in datasplit:
      # Prepare variables and create a mask of missing angles (padded with zeros)
      # Note the mask is repeated in the last dimension to match the sin/cos represenation.
      seqs = batch.seqs.to(device)
      true_angles_sincos = scn.structure.trig_transform(batch.angs).to(device)
      mask = (batch.angs.ne(0)).unsqueeze(-1).repeat(1, 1, 1, 2)

      # Make predictions and optimize
      pred_angles_sincos = model(seqs)
      loss = mse_loss(pred_angles_sincos[mask], true_angles_sincos[mask])
      total += loss
      n += 1
  return torch.sqrt(total/n)

for epoch in range(20):
  print(f"Epoch {epoch}")
  progress_bar = tqdm(total=len(d['train']), smoothing=0)
  for batch in d['train']:
      # Prepare variables and create a mask of missing angles (padded with zeros)
      # Note the mask is repeated in the last dimension to match the sin/cos represenation.
      seqs = batch.seqs.to(device)
      true_angles_sincos = scn.structure.trig_transform(batch.angs).to(device)
      mask = (batch.angs.ne(0)).unsqueeze(-1).repeat(1, 1, 1, 2)

      # Make predictions and optimize
      pred_angles_sincos = seqonly_model(seqs)
      loss = mse_loss(pred_angles_sincos[mask], true_angles_sincos[mask])
      loss.backward()
      torch.nn.utils.clip_grad_norm_(seqonly_model.parameters(), 2)
      optimizer.step()

      # Housekeeping
      batch_losses.append(float(loss))
      progress_bar.update(1)
      progress_bar.set_description(f"\rRMSE Loss = {np.sqrt(float(loss)):.4f}")

  # Evaluate the model's performance on train-eval, downsampled for efficiency
  epoch_training_losses.append(evaluate(seqonly_model, d['train-eval']))
  # Evaluate the model's performance on various validation sets
  epoch_validation10_losses.append(evaluate(seqonly_model, d['valid-10']))
  epoch_validation90_losses.append(evaluate(seqonly_model, d['valid-90']))
  print(f"\tTrain-eval loss = {epoch_training_losses[-1]:.4f}")
  print(f"\tValid-10   loss = {epoch_validation10_losses[-1]:.4f}")
  print(f"\tValid-90   loss = {epoch_validation90_losses[-1]:.4f}")

# Finally, evaluate the model on the test set
print(f"Test loss = {evaluate(seqonly_model, d['test']):.4f}")

plt.plot(np.sqrt(np.asarray(batch_losses)), label='batch loss')
plt.ylabel("RMSE")
plt.xlabel("Step")
plt.title("Training Loss over Time")
plt.show()

# While the above plot demonstrates each batch's loss during training,
# the plot below shows the performance of the model on several data splits
# at the *end* of each epoch.
plt.plot(epoch_training_losses, label='train-eval')
plt.plot(epoch_validation10_losses, label='valid-10')
plt.plot(epoch_validation90_losses, label='valid-90')
plt.ylabel("RMSE")
plt.xlabel("Epoch")
plt.title("Training and Validation Losses over Time")
plt.legend()
plt.show()

"""### 4.1.1 A note on visualizing predictions
Although this is somewhat covered in section 3, **"Visualizing Structures"**, this section makes it more clear how to visualize structures that are predicted by model.

In most cases, you'll want to use `scn.BatchedStructureBuilder` which takes as arguments:

1. A tensor (batch x L) of integers representing the protein sequences in the batch. This information is provided while iterating using the given DataLoaders. (See `seq` variable in training loops above.)
2. A tensor (batch x L x NUM_ANGLES) of floating point numbers in $[-\pi, \pi]$ representing the predicted angles for each residue of each amino acid in the protein.

In cases like those in Sections 4.1 and 4.2, you may have a model trained to predict the sin/cos values of a set of angles. Simply transform these values into the actual angle tensor (see `scn.structure.inverse_trig_transform`) before providing them to the `BatchedStructureBuilder`.

"""

def build_visualizable_structures(model, data, mode=None):
  """Build visualizable structures for one batch of model's predictions on data."""
  with torch.no_grad():
    for batch in data:
      if mode == "seqs":
        model_input = batch.seqs.to(device)
      elif mode == "pssms":
        model_input = batch.seq_evo_sec.to(device)

      # Make predictions for angles, and construct 3D atomic coordinates
      predicted_angles_sincos = model(model_input)
      predicted_angles = inverse_trig_transform(predicted_angles_sincos)
      sb = scn.BatchedStructureBuilder(batch.int_seqs, predicted_angles.cpu())
      break
  return sb

s = build_visualizable_structures(seqonly_model, d["train"], mode="seqs")

s.to_3Dmol(2)

"""## 4.2 Training with Sequences, PSSMs, Secondary Structures, and Information Content

**How can we improve upon the previous model?** The answer: more data and a more capable model!

This example does the following:


*   Adds PSSMs, secondary structure, and information content to the model input by accessing the `batch.seq_evo_sec` attribute,
*   Uses the smallest version of the CASP 12 dataset (CASP 12, 30% thinning), and
*  Increases the size of the model by increasing the hidden state dimension to 1024.


**What's a PSSM?**

A PSSM is a Position Specific Scoring Matrix (often called a Position Weight Matrix or Position Probability Matrix in DNA-speak).

You may be familiar with images like the one below which visualizes how frequently a particular sequence element is observed at a given position when comparing many similar sequences in an alignment. The image and matrix shown below contain such data for DNA sequences. We are simply utilizing the same kind of data, but for protein sequences!
You may be familiar with images like the one below which visualizes how frequently a particular sequence element is observed at a given position when comparing many similar sequences in an alignment. The image and matrix shown below summarize such data for DNA sequences. We are simply utilizing the same kind of data, but for protein sequences!

![](https://upload.wikimedia.org/wikipedia/commons/8/85/LexA_gram_positive_bacteria_sequence_logo.png)



![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6940f505c03de60ce26b4f89b9f1e9f867b714c8)


*Images by Gnomehacker at English Wikipedia, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=34623350)*
"""

# Load the data in the appropriate format
d = scn.load("debug",
            #  casp_version=12,
            #  thinning=30,
             with_pytorch="dataloaders",
             aggregate_model_input=True,
             batch_size=16,
             dynamic_batching=True)

pssm_model = models.PSSMProteinRNN(size=512,
                                   n_layers=4,
                                   bidirectional=True,
                                   device=device)
pssm_model.to(device)

optimizer = torch.optim.Adam(pssm_model.parameters())
batch_losses = []
epoch_training_losses = []
epoch_validation10_losses = []
epoch_validation90_losses = []
mse_loss = torch.nn.MSELoss()

def evaluate_pssm(model, datasplit):
  """Evaluate a model with (input:PSSM, output:sin/cos represented angles
     [-1,1]) on LDRMSD."""
  total = 0.0
  n = 0
  with torch.no_grad():
    for batch in datasplit:
      # Prepare variables and create a mask of missing angles (padded with zeros)
      # Note the mask is repeated in the last dimension to match the sin/cos represenation.
      seq_evo_sec = batch.seq_evo_sec.to(device)
      true_angles_sincos = scn.structure.trig_transform(batch.angs).to(device)
      mask = (batch.angs.ne(0)).unsqueeze(-1).repeat(1, 1, 1, 2)

      # Make predictions and optimize
      pred_angles_sincos = model(seq_evo_sec)
      loss = mse_loss(pred_angles_sincos[mask], true_angles_sincos[mask])
      total += loss
      n += 1
  return torch.sqrt(total/n)

for epoch in range(20):
  print(f"Epoch {epoch}")
  progress_bar = tqdm(total=len(d['train']), smoothing=0)
  for batch in d['train']:
      # Prepare variables and create a mask of missing angles (padded with zeros)
      # Note the mask is repeated in the last dimension to match the sin/cos represenation.
      seq_evo_sec = batch.seq_evo_sec.to(device)
      true_angles_sincos = scn.structure.trig_transform(batch.angs).to(device)
      mask = (batch.angs.ne(0)).unsqueeze(-1).repeat(1, 1, 1, 2)

      # Make predictions and optimize
      pred_angles_sincos = pssm_model(seq_evo_sec)
      loss = mse_loss(pred_angles_sincos[mask], true_angles_sincos[mask])
      loss.backward()
      torch.nn.utils.clip_grad_norm_(pssm_model.parameters(), 2)
      optimizer.step()

      # Housekeeping
      batch_losses.append(float(loss))
      progress_bar.update(1)
      progress_bar.set_description(f"\rRMSE Loss = {np.sqrt(float(loss)):.4f}")

  # Evaluate the model's performance on train-eval, downsampled for efficiency
  epoch_training_losses.append(evaluate_pssm(pssm_model, d['train-eval']))
  # Evaluate the model's performance on various validation sets
  epoch_validation10_losses.append(evaluate_pssm(pssm_model, d['valid-10']))
  epoch_validation90_losses.append(evaluate_pssm(pssm_model, d['valid-90']))
  print(f"\tTrain-eval loss = {epoch_training_losses[-1]:.4f}")
  print(f"\tValid-10   loss = {epoch_validation10_losses[-1]:.4f}")
  print(f"\tValid-90   loss = {epoch_validation90_losses[-1]:.4f}")

# Finally, evaluate the model on the test set
print(f"Test loss = {evaluate_pssm(pssm_model, d['test']):.4f}")

plt.plot(np.sqrt(np.asarray(batch_losses)))
plt.ylabel("RMSE")
plt.xlabel("Step")
plt.title("Training Loss over Time")
plt.show()

# While the above plot demonstrates each batch's loss during training,
# the plot below shows the performance of the model on several data splits
# at the *end* of each epoch.
plt.plot(epoch_training_losses, label='train-eval')
plt.plot(epoch_validation10_losses, label='valid-10')
plt.plot(epoch_validation90_losses, label='valid-90')
plt.ylabel("RMSE")
plt.xlabel("Epoch")
plt.title("Training and Validation Losses over Time")
plt.legend()
plt.show()

# Using the same function we defined in 4.1.1, we can visualize some of the
# model's predictions on the training set to see how well it learned
s = build_visualizable_structures(pssm_model, d["train"], mode="pssms")

s.to_3Dmol(0)

"""## 4.3 Coordinate-based loss functions (DRMSD)
You can also train models that predict angles before converting these angles to the 3D full-atom protein structures they represent using `scn.BatchedStructureBuilder`.

This model can be trained with Length-normalized Distance-based Root Mean Square Deviation (LDRMSD, `scn.examples.losses.compute_batch_drmsd`), a *differentiable* metric for comparing two sets of atomic coordinates that need not be aligned first.

*Note that using this strategy is unoptimized and is much slower than training on angles alone. Converting angles to coordinates is computationally expensive. A parallel implementation will be provided in future releases.*
"""

# Load the data in the appropriate format
d = scn.load("debug",
            #  casp_version=12,
            #  thinning=30,
             with_pytorch="dataloaders",
             aggregate_model_input=True,
             batch_size=16,
             dynamic_batching=True)

# In this instance we can predict the angles directly, since they will not be used for training
pssm_coord_model = models.PSSMProteinRNN(size=512,
                                   n_layers=4,
                                   bidirectional=True,
                                   sincos_output=True,
                                   device=device)
pssm_coord_model.to(device)

optimizer = torch.optim.Adam(pssm_coord_model.parameters())
batch_losses = []
epoch_training_losses = []
epoch_validation10_losses = []
epoch_validation90_losses = []

def evaluate_pssm_coord(model, datasplit):
  """Evaluate a model with (input:PSSM, output:angles [-pi,pi]) on LDRMSD."""
  total = 0.0
  n = 0
  with torch.no_grad():
    for batch in datasplit:
      # Prepare variables
      model_input = batch.seq_evo_sec.to(device)

      # Make predictions for angles, and construct 3D atomic coordinates
      predicted_angles = model(model_input)
      predicted_angles = inverse_trig_transform(predicted_angles)
      sb = scn.BatchedStructureBuilder(batch.int_seqs, predicted_angles.cpu())
      predicted_coords = sb.build()
      loss = losses.compute_batch_drmsd(batch.crds, predicted_coords,
                                        batch.int_seqs)
      total += loss
      n += 1
  return total/n

for epoch in range(20):
  print(f"Epoch {epoch}")
  progress_bar = tqdm(total=len(d['train']), smoothing=0)
  for batch in d['train']:
      # Prepare variables
      model_input = batch.seq_evo_sec.to(device)

      # Predict the angles in sin/cos format before transforming to radians
      predicted_angles_sincos = pssm_coord_model(model_input)
      predicted_angles = inverse_trig_transform(predicted_angles_sincos)

      # BatchedStructureBuilder can be used to generate atomic structures for
      # a given batch of proteins represented as angles (batch x L x NUM_ANGLES)
      sb = scn.BatchedStructureBuilder(batch.int_seqs, predicted_angles.cpu())
      predicted_coords = sb.build()
      loss = losses.compute_batch_drmsd(batch.crds, predicted_coords,
                                        batch.int_seqs)
      loss.backward()
      torch.nn.utils.clip_grad_norm_(pssm_coord_model.parameters(), 2)
      optimizer.step()

      # Housekeeping
      batch_losses.append(float(loss))
      progress_bar.update(1)
      progress_bar.set_description(f"\rLDRMSD Loss = {float(loss):.4f}")

  # Evaluate the model's performance on train-eval, downsampled for efficiency
  epoch_training_losses.append(evaluate_pssm_coord(pssm_coord_model, d['train-eval']))
  # Evaluate the model's performance on various validation sets
  epoch_validation10_losses.append(evaluate_pssm_coord(pssm_coord_model, d['valid-10']))
  epoch_validation90_losses.append(evaluate_pssm_coord(pssm_coord_model, d['valid-90']))
  print(f"\tTrain-eval loss = {epoch_training_losses[-1]:.4f}")
  print(f"\tValid-10   loss = {epoch_validation10_losses[-1]:.4f}")
  print(f"\tValid-90   loss = {epoch_validation90_losses[-1]:.4f}")

# Finally, evaluate the model on the test set
print(f"Test loss = {evaluate_pssm_coord(pssm_coord_model, d['test']):.4f}")

plt.plot(np.sqrt(np.asarray(batch_losses)))
plt.ylabel("LDRMSD")
plt.xlabel("Step")
plt.title("Training Loss over Time")
plt.show()

# While the above plot demonstrates each batch's loss during training,
# the plot below shows the performance of the model on several data splits
# at the *end* of each epoch.
plt.plot(epoch_training_losses, label='train-eval')
plt.plot(epoch_validation10_losses, label='valid-10')
plt.plot(epoch_validation90_losses, label='valid-90')
plt.ylabel("LDRSDM")
plt.xlabel("Epoch")
plt.title("Training and Validation Losses over Time")
plt.legend()
plt.show()

# Using the same function we defined in 4.1.1, we can visualize some of the
# model's predictions on the training set to see how well it learned
s = build_visualizable_structures(pssm_coord_model, d["train"], mode="pssms")

s.to_3Dmol(0)

"""We know a lot of the prediction visualizations in this notebook look fairly awful. We know, and it makes us sad, too.

There's much work to be done in model training. We look forward to seeing what the community can do with this dataset!

# 5. Reproducing and Extending SidechainNet

## 5.1 Generating a custom version of SidechainNet

If you would like to modify SidechainNet's organization or composition, we provide several tools to help. You'll be able to specify which proteins to download and what dataset split to assign them to. You may even construct an arbitrary number of validation splits or add proteins that were previously excluded from ProteinNet/SidechainNet (e.g., recently released protein structures).

The only requirement is that you provide a list of ProteinNet-formatted IDs.

The ProteinNet format is as follows:
> `<class>#<pdb_id>_<chain/model_number>_<chain_id>`

*   `class`: Determines dataset split assignment.
   - If `class` is empty *and the `#` is omitted*, then the ID is treated as a training set entry.
   - If `class` is an integer (`3,10,70,49` etc.), then the ID is assigned to a validation set named `'valid-class'`, (i.e. `'valid-3'`).
   - If `class` is any of `'TBM, FM, TBM-hard, FM-hard, Unclassified'`, then the ID is assigned to the test set.
*   `<pdb_id>`: 4-letter RSCB PDB identifier (`1A9U`)
*   `<chain_number>`: integer describing the chain/model number/coordinate set entry (using 1 is a reasonable default if you are unsure)
*   `<chain_id>`: 1-letter chain name (`A`, `B`, etc.)

### 5.1.1 Creating a list of ProteinNet IDs

Let's say we would like to generate a dataset that utilizes:

* the 30% training set thinning from ProteinNet's CASP12 dataset (for this example we will simply load the first 50 IDs),
* only the `'valid-10'` and `'valid-90'` validation sets from CASP12, and  
* the testing set from CASP11.

This way, we can see how a model might perform retroactively on CASP11 if we had access to CASP12 data.  We also want to update SidechainNet to include a recently published protein structure, in this case an Immunity-related GTPase from a [house mouse subspecies](https://www.google.com/search?q=%22Mus+musculus+molossinus%22&newwindow=1&sxsrf=ALeKk01Fq0f1pNwSynwRs74GLPwdOHf4jQ:1619451753106&source=lnms&tbm=isch&sa=X&ved=2ahUKEwjv0uXzn5zwAhVWJzQIHcNvC58Q_AUoAXoECAEQAw&biw=1672&bih=1338) ([7C3K](https://www.rcsb.org/structure/7C3K), chain A, coordinate set 1). In ProteinNet format, this entry would be `'7C3K_1_A'`.

To do this, the first step is to compile a list of relevant ProteinNet IDs for our dataset. We can write them programmatically/by hand, or we can utilize `sidechainnet.get_proteinnet_ids()`, which loads the ProteinNet IDs in use by previous versions of ProteinNet.
"""

import sidechainnet as scn

# Acquire the first 50 IDs from the correct ProteinNet CASP version
training_ids = scn.get_proteinnet_ids(casp_version=12, split="train", thinning=30)[:50]

# Add a new protein ID, not previously included in ProteinNet/SidechainNet
training_ids = ['7C3K_1_A'] + training_ids
training_ids[:5]

valid_ids = scn.get_proteinnet_ids(casp_version=12, split="valid-10")
valid_ids += scn.get_proteinnet_ids(casp_version=12, split="valid-90")
valid_ids[:5]

test_ids = scn.get_proteinnet_ids(casp_version=12, split="test")
test_ids[:5]

"""### 5.1.2 Using `scn.create_custom` to generate the data
Now we are ready to generate our custom SidechainNet dataset with `scn.create_custom`. Please note that due to constraints on computing resources (storage, memory, n_cpus), we strongly recommend that this procedure be completed outside of Colab on a machine with multiple available CPUs.
"""

d = scn.create_custom(pnids=training_ids + valid_ids + test_ids,
                      output_filename="custom01.pkl",
                      short_description="CASP12 data with the exception of CASP11's test set.")

"""Right away, we are ready to investigate the data, which has been returned as a Python dictionary organized by dataset split. We can also check that our training set now includes the just-released protein structure 7C3K."""

print(d['description'])
d['settings']

'7C3K_1_A' in d['train']['ids']

"""### 5.1.3 Loading the custom dataset in alternative formats (PyTorch)
Now that the data has been generated, we can load the data in a format that is more amenable for training machine learning models. For instance, we will now load the data with PyTorch Dataloaders, which handles dataset batching, padding, and organization.
"""

dl = scn.load(local_scn_path="sidechainnet_data/custom01.pkl",
              with_pytorch="dataloaders",
              batch_size=4,
              dynamic_batching=False)
dl.keys()

training_dl = dl['train']
valid_dl = dl['valid-10']
testing_dl = dl['test']

train_batch = next(iter(training_dl))
train_batch._fields

train_batch.pids

train_batch.angs.shape, train_batch.crds.shape

train_batch.ress

train_batch.int_seqs

"""## 5.2 Reproducing SidechainNet with `scn.create`

In the case that the user would like to generate SidechainNet data from scratch, please
use `scn.create`.


ProteinNet and SidechainNet are organized into several different datasets depending on the
CASP version number and the training set thinning. Simply call `scn.create` and SidechainNet
will download a minimally preprocessed version of ProteinNet into
`sidechainnet/resources/proteinnet_parsed` (this need not be accessed directly by the user).
 Then, SidechainNet will download the relevant proteins from RCSB PDB and unify this data
 with several dataset features from ProteinNet (e.g., secondary structury and
position-specific scoring matrices). Finally, the data can be loaded with `scn.load`.

As with generating a custom version of SidechainNet, reproducing SidechainNet is also
computationally expensive and we suggest performing this operation outside of Colab.
"""

scn.create?

scn.create(casp_version=12, thinning=30)

data = scn.load(casp_version=12, thinning=30)

"""# Getting in touch
If you have any questions, concerns, or comments, please reach out to us on the [GitHub repository page](https://github.com/jonathanking/sidechainnet). Thank you!
"""

